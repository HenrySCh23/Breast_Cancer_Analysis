# Breast Cancer Prediction - Exploratory Data Analysis (EDA)

## ğŸ“Œ Project Overview

This project focuses on Exploratory Data Analysis (EDA) of the Breast Cancer Prediction Dataset to identify patterns and insights in the data. The dataset contains various biometric measurements of tumors and a diagnosis label (Benign or Malignant). The analysis aims to help understand the dataset's structure, preprocess the data, and explore potential predictive patterns.

## ğŸ“Š Data Source

This dataset originates from the University of Wisconsin Hospitals, Madison and was created by Dr. William H. Wolberg. The dataset is publicly available on Kaggle:

ğŸ”— Breast Cancer Prediction Dataset - Kaggle : https://www.kaggle.com/datasets/merishnasuwal/breast-cancer-prediction-dataset

âš ï¸ Note: The dataset is NOT included in this repository. Please download it from the Kaggle link above before running the analysis.

## ğŸ”¬ Methods & Analysis

The following techniques are applied to analyze and preprocess the dataset:

### ğŸ“Š Step 1: Exploratory Data Analysis (EDA)

- Reading the dataset (pandas)

- Checking for missing values (none in this case)

- Handling categorical data (mapping 0 = Malignant, 1 = Benign)

- Understanding feature distributions

- ğŸ“ **Notebook:** [01_EDA.ipynb](notebooks/01_EDA.ipynb)

### ğŸ“Š Step 2: Comparing Dimensionality Reduction Techniques

| **Method** | **Components** | **Used For** |
|------------|--------------|--------------|
| PCA | 2D | Train-Test Splitting & Visualization |
| MDS | 2D,3D | Visualization |
| t-SNE | 2D,3D | Visualization |
| UMAP | 2D,3D | Visualization |

- ğŸ“ **Notebook:** [02_All_Dim_Reduction.ipynb](notebooks/02_All_Dim_Reduction.ipynb)
- **Includes:** Scatter plots & visual comparisons of all techniques.

### ğŸ“Š Step 3: Dimensionality reduction - Model Comparison

- The **best dimensionality reduction techniques** are selected based on:
  - **Silhouette Score**
  - **Stress (for MDS)**
  - **Correlation with original distances**
  - **Classification Accuracy (Random Forest)**
- The final **Random Forest classification model** is trained on:
  - **PCA (2D) â†’ Performed Poorly**
  - **MDS (3D) â†’ Best Performance**
  - **t-SNE (2D) â†’ Near-optimal Performance**
  - **UMAP (3D) â†’ Strong Alternative**

### **ğŸ“Š Final Accuracy Results:**
| **Method** | **Classification Accuracy (%)** |
|------------|-------------------------------|
| **PCA (2D)** | **54.4%** âŒ |
| **t-SNE (2D)** | **92.9%** âœ… |
| **UMAP (3D)** | **91.8%** âœ… |
| **MDS (3D)** | **93.5%** âœ… |

ğŸ“ **Notebook:** [03_Best_Models_DR.ipynb](notebooks/03_Best_Models_DR.ipynb)


## ğŸ“Œ Best Dimensionality Reduction: MDS (3D â†’ 2D)
Below is the final decision boundary for MDS (3D reduced to 2D), which gave the highest classification accuracy.

![MDS Decision Boundary](images/MDS_3D_to_2D_Decision_Boundary.png)

---

## **ğŸ›  How to Run This Project**

### **1ï¸âƒ£ Install Requirements**

```bash
pip install -r requirements.txt
```



1. Download Dataset

- Go to Kaggle

- Download the dataset and place it in the project directory

2. Run the Analysis

- Ensure you have the necessary dependencies installed:

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings

# Load dataset
data = pd.read_csv("breast_cancer.csv")

# Display first 5 rows
print(data.head())
```

## ğŸ“œ License

This repository is licensed under the Apache 2 License. You are free to use and modify the code but must provide proper attribution.

## ğŸ‘¨â€ğŸ’» Author & Acknowledgments

This project was developed by Henry Serpa, as part of practicing Data Science concepts.

## ğŸ™ Special thanks to:

- Dr. William H. Wolberg (Dataset Creator)

- Kaggle Community for hosting the dataset

- Open-source contributors of Python libraries (pandas, seaborn, matplotlib, etc.)

## âœ… Future Updates

ğŸ“Œ Classification models

ğŸ“Œ Machine Learning Models (Logistic Regression, Random Forest, etc.)

Feel free to â­ star this repository if you found it helpful!


